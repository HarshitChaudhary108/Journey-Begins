{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc18308f-9f1c-47d0-8ce1-b27e100d700c",
   "metadata": {},
   "source": [
    "## Q1. What is a projection and how is it used in PCA?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e92f886-406f-4db1-8c1f-c28024534b60",
   "metadata": {},
   "source": [
    "#### In geometry and linear algebra, a projection is essentially the process of translating points from a high-dimensional space onto a lower-dimensional subspace. Imagine shining a flashlight on a 3D object to cast a 2D shadow ‚Äî the shadow is the projection.\n",
    "## How is it used in PCA :\n",
    "#### PCA uses projection to reduce dimensionality while preserving as much variance (information) as possible:\n",
    "#### 1. Find Principal Components (PCs)\n",
    "- These are the directions (vectors) in which the data varies the most.\n",
    "- First PC captures maximum variance, second PC captures the next highest (orthogonal to the first), and so on.\n",
    "#### 2. Project the Data\n",
    "- Each data point is projected onto these principal components.\n",
    "- Instead of expressing the point in terms of its original features, it‚Äôs now represented using a reduced number of PCs.\n",
    "#### 3. Dimensionality Reduction\n",
    "- If you keep only the top k principal components, you reduce the dataset from n dimensions to k, while retaining most of the \"shape\" or structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb142d7e-7de5-43ca-a7b9-ae6db1cb735e",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e1f57-1c80-4c03-aaa1-12a081b1d0f3",
   "metadata": {},
   "source": [
    "## Q2. How does the optimization problem in PCA work, and what is it trying to achieve?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ad75fb-ef25-413e-9ca3-7037ca420cc8",
   "metadata": {},
   "source": [
    "#### Goal of PCA‚Äôs Optimization :\n",
    "- PCA wants to answer this: ‚ÄúWhat are the best directions to project the data so that we preserve the maximum variance?‚Äù\n",
    "- Why variance? Because variance captures information, structure, and spread in the dataset. High variance means the data points are well spread out in that direction ‚Äî so PCA prioritizes keeping that.\n",
    "#### HOW IT WORKS ? :\n",
    "#### Input:\n",
    "##### A dataset ùëã‚ààùëÖ ùëõ√óùëë, where:\n",
    "- ùëõ = number of samples\n",
    "- ùëë = number of original features\n",
    "##### Assume the data is centered, i.e. mean = 0\n",
    "####\n",
    "#### Objective:\n",
    "##### Find a vector ùë§‚ààùëÖùëë (a direction) such that the variance of the data projected onto ùë§ is maximized.\n",
    "##### Here ùëÜ is the covariance matrix of the data.\n",
    "##### The constraint ‚à•ùë§‚à• = 1 ensures we‚Äôre only choosing direction ‚Äî not magnitude.\n",
    "####\n",
    "#### Solution:\n",
    "##### This is solved via eigen decomposition.\n",
    "##### The optimal ùë§ is the eigenvector of ùëÜ corresponding to the largest eigenvalue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5764ade0-5a6e-4683-b93b-631c23a60062",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d64537d-a328-48d5-8ffe-9769fcd61add",
   "metadata": {},
   "source": [
    "## Q3. What is the relationship between covariance matrices and PCA?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292aad96-0570-4c95-b198-3812da03ab0d",
   "metadata": {},
   "source": [
    "#### Covariance Matrix: The DNA of PCA\n",
    "- At the heart of PCA lies the covariance matrix, a square matrix that reveals how features in your dataset vary with respect to each other.\n",
    "## PCA‚Äôs Job: Decode That Matrix\n",
    "#### PCA transforms your dataset into new axes by solving the covariance matrix:\n",
    "#### 1. Compute the Covariance Matrix ùëÜ\n",
    "#### 2. Diagonalize ùëÜ\n",
    "- Find the eigenvectors and eigenvalues of ùëÜ.\n",
    "- Eigenvectors = principal components (directions of maximum variance)\n",
    "- Eigenvalues = amount of variance each component captures\n",
    "#### 3. Construct the PCA Basis\n",
    "- Arrange the eigenvectors in descending order of eigenvalues.\n",
    "- These vectors become the new axes to project your data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9071e9-c31a-46b3-9713-15eecb679af4",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9db3307-2119-4634-9bfe-ea8479d3f0a9",
   "metadata": {},
   "source": [
    "## Q4. How does the choice of number of principal components impact the performance of PCA?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e116991-cde0-490c-a852-d434770510ce",
   "metadata": {},
   "source": [
    "## 1. Information Retention vs Compression\n",
    "#### More PCs: \n",
    "- Capture more variance ‚Üí richer representation of data\n",
    "- Fewer PCs: Leaner data ‚Üí better generalization, but risk of losing meaningful signal\n",
    "- Trade-off: Retaining ~95% variance is often a sweet spot, but context matters\n",
    "####\n",
    "## 2. Model Accuracy\n",
    "- In models like KNN, a well-chosen number of PCs improves accuracy by focusing on the most informative directions.\n",
    "- Too few PCs ‚Üí underfitting (important patterns lost)\n",
    "- Too many PCs ‚Üí overfitting (model picks up on noise)\n",
    "####\n",
    "## 3. Computational Efficiency\n",
    "- Fewer PCs mean faster training and inference.\n",
    "- Useful in real-time systems or when working with large datasets.\n",
    "#### \n",
    "## 4. Noise Reduction\n",
    "- PCA inherently filters out dimensions with low variance (likely noise).\n",
    "- Proper selection helps clarify signal from background clutter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c42dcc5-7bae-4c19-bbc3-1ae778aabd45",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f99b5-2b31-41c2-acf2-8a8eedc98eb9",
   "metadata": {},
   "source": [
    "## Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836c15a-f107-4576-a763-2b0c3feb218b",
   "metadata": {},
   "source": [
    "#### PCA transforms your original features into principal components (PCs) ‚Äî linear combinations of the original features ordered by variance captured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f41aa-d24b-4a22-a87f-ad2bb2bce4f6",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction :\n",
    "- PCA lets you represent your data with fewer synthetic features (PCs) that capture most of the variance.\n",
    "- This helps reduce noise and redundancy from highly correlated or low-impact original features.\n",
    "#### Variance Prioritization :\n",
    "- Features contributing to the top PCs are considered more informative.\n",
    "- You can analyze the loading scores - pca.explained_variance_ratio_ - to decide which original features to keep.\n",
    "#### Unsupervised Preprocessing\n",
    "- PCA doesn‚Äôt depend on target labels, making it perfect for unsupervised feature selection.\n",
    "- It gives a ‚Äúclean slate‚Äù view of which directions in the data are most active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da3a3b4-a703-42fc-8152-e47022dc0b02",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7410852-6a72-4de8-ba72-1e78889d5090",
   "metadata": {},
   "source": [
    "## Q6. What are some common applications of PCA in data science and machine learning?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe5ba6-80db-4c2b-90e7-37a3820b51c6",
   "metadata": {},
   "source": [
    "#### 1. Dimensionality Reduction for Modeling\n",
    "#### 2. Data Visualization (high dimension to lower dimension)\n",
    "#### 3. Noise Filtering - PCA filters out features with low variance, often reducing noise in sensor data, text embeddings, or biological data\n",
    "#### 4. Anomaly Detection - By compressing the feature space, PCA makes outliers more visible.\n",
    "#### 5. Image Compression & Recognition - PCA reduces image size while preserving essential structural patterns (e.g., Eigenfaces concept)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a8cf64-f57a-48a1-b2fe-83e205afd82e",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b896d24-e629-4c38-ac47-a1b596ae724a",
   "metadata": {},
   "source": [
    "## Q7.What is the relationship between spread and variance in PCA?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5446fa-aeba-4f26-b9ab-5513fd4f707d",
   "metadata": {},
   "source": [
    "#### \"Spread\" refers to how data points are distributed along a direction or axis. A greater spread means that points are more scattered, and that axis captures more variation in the dataset.\n",
    "#### PCA‚Äôs Objective: Maximize Spread = Maximize Variance\n",
    "#### In PCA:\n",
    "- The goal is to find axes (principal components) where the data shows the maximum spread.\n",
    "- These axes are chosen by calculating the variance along every possible direction, and keeping the ones with the highest variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c748d896-a1e3-4e79-8337-75c071d8ad64",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f18d3-01e5-46ef-af2a-ef5bc93dae49",
   "metadata": {},
   "source": [
    "## Q8. How does PCA use the spread and variance of the data to identify principal components?\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a73cf03-34f1-44cd-b35c-4c279c8bf8d3",
   "metadata": {},
   "source": [
    "#### 1. Compute Covariance Matrix\n",
    "#### 2. Eigen-Decomposition of ùëÜ :\n",
    "- Extract eigenvectors (directions of variance)\n",
    "- Extract eigenvalues (amount of variance along those directions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d917e9-6eef-43bb-a8f1-6e62a02aa5c0",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad1fc4-7443-41f8-aaa4-d17f1e0500b3",
   "metadata": {},
   "source": [
    "## Q9. How does PCA handle data with high variance in some dimensions but low variance in others?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4f1f1-44cc-4423-a33c-6f3bda0824f7",
   "metadata": {},
   "source": [
    "#### The covariance matrix ùëÜ captures how features vary together:\n",
    "- High variance ‚Üí large entries\n",
    "- Low variance ‚Üí smaller influence\n",
    "##### So eigenvectors aligned with high-variance directions will get higher eigenvalues ‚Äî and be prioritized as principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50be1636-6e89-4899-a9b0-7b651029d0e0",
   "metadata": {},
   "source": [
    "- High-variance dimensions: preserved in early components\n",
    "- Low-variance dimensions: relegated to later components or discarded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
