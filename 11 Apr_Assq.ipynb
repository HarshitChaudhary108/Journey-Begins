{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908ccc4c-2be9-4bb6-beb2-bea9fbfdef45",
   "metadata": {},
   "source": [
    "## Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbce70c-7d51-4505-8a45-bd0af2941ed0",
   "metadata": {},
   "source": [
    "## Answer \n",
    "#### Ensemble techniques in machine learning involve combining multiple models to improve the overall performance and accuracy of predictions. The idea is that by aggregating the strengths of various models, the ensemble can achieve better results than any single model alone.\n",
    "### There are several common types of ensemble techniques:\n",
    "#### Bagging (Bootstrap Aggregating): \n",
    "- This method involves training multiple versions of a model on different subsets of the data (created using bootstrapping), and then aggregating their predictions. Random Forest is a well-known example of a bagging technique.\n",
    "#### Boosting: \n",
    "- In boosting, models are trained sequentially, with each new model focusing on correcting the errors made by the previous models. The final prediction is a weighted sum of the individual models' predictions. Examples of boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79bcef9-a259-4de2-81c5-5544fc32b81b",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d496c067-cc72-4a57-94f1-fd90314eb846",
   "metadata": {},
   "source": [
    "##  Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ea7f31-f329-419d-bca6-d833e10b2e05",
   "metadata": {},
   "source": [
    "## Answer\n",
    "#### Ensemble techniques are used in machine learning because they can significantly enhance the performance and robustness of models. Here are some reasons why these techniques are so valuable:\n",
    "#### Improved Accuracy: \n",
    "- By combining multiple models, ensemble techniques can produce more accurate and reliable predictions than a single model. This is because different models can capture different aspects of the data, and their combined predictions often result in a more comprehensive understanding.\n",
    "#### Reduced Overfitting: \n",
    "- Ensemble methods, like bagging and boosting, can help reduce overfitting by averaging out the errors of individual models. This leads to better generalization to unseen data.\n",
    "#### Handling Complex Problems: \n",
    "- Some problems are too complex for a single model to handle effectively. Ensembles can tackle these problems by leveraging the strengths of different models and addressing their individual weaknesses.\n",
    "#### Bias-Variance Tradeoff: \n",
    "- Ensembles can help manage the bias-variance tradeoff more effectively. By averaging the predictions of multiple models, ensembles can reduce the variance without significantly increasing the bias.\n",
    "#### Flexibility: \n",
    "- Ensembles can be constructed from different types of models (e.g., decision trees, neural networks, support vector machines), allowing for a flexible and diverse approach to problem-solving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7599af-d7bd-49dd-aed8-11efaf24a2bb",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9ef75-f586-485d-89f0-b4628b906080",
   "metadata": {},
   "source": [
    "## Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e952d7-fac0-4563-b3b1-5d004627e9cd",
   "metadata": {},
   "source": [
    "## Answer \n",
    "#### Bagging, short for Bootstrap Aggregating, is an ensemble technique in machine learning designed to improve the stability and accuracy of models, particularly those prone to overfitting, such as decision trees. Here's how it works:\n",
    "#### Bootstrap Sampling: \n",
    "- Multiple subsets of the original training dataset are created using a process called bootstrapping. Bootstrapping involves randomly selecting data points with replacement, meaning some data points may appear more than once, while others may not appear at all.\n",
    "#### Training Models: \n",
    "- Separate models are trained on each of these bootstrapped subsets. These models are usually of the same type (e.g., decision trees) but may produce slightly different results due to the variations in the training data.\n",
    "#### Aggregating Predictions: \n",
    "- Once all models are trained, their predictions are combined to make the final prediction. For classification problems, the final prediction is typically determined by majority voting (the class predicted by most models is chosen). For regression problems, the predictions are averaged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dccf82-ba64-4778-994c-b05300096623",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072c6177-89ed-42e9-b2de-6cc58f45e205",
   "metadata": {},
   "source": [
    "## Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785760a4-3c59-4b78-ae96-e1818a9d9cf2",
   "metadata": {},
   "source": [
    "## Answer \n",
    "#### Boosting is another powerful ensemble technique in machine learning that focuses on improving the performance of weak models. The main idea behind boosting is to sequentially train a series of models, where each model attempts to correct the errors made by the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23f35ad-ef5e-411a-9638-962d567480d8",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0866c483-cda6-4f24-ad58-bc5d42878c73",
   "metadata": {},
   "source": [
    "##  Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef859c-43e5-44f4-8a58-ba196ad62410",
   "metadata": {},
   "source": [
    "## Answer\n",
    "#### Improved Accuracy: \n",
    "- By combining multiple models, ensemble techniques can produce more accurate and reliable predictions than a single model. This is because different models can capture different aspects of the data, and their combined predictions often result in a more comprehensive understanding.\n",
    "#### Reduced Overfitting: \n",
    "- Ensemble methods, like bagging and boosting, can help reduce overfitting by averaging out the errors of individual models. This leads to better generalization to unseen data.\n",
    "#### Handling Complex Problems: \n",
    "- Some problems are too complex for a single model to handle effectively. Ensembles can tackle these problems by leveraging the strengths of different models and addressing their individual weaknesses.\n",
    "#### Bias-Variance Tradeoff: \n",
    "- Ensembles can help manage the bias-variance tradeoff more effectively. By averaging the predictions of multiple models, ensembles can reduce the variance without significantly increasing the bias.\n",
    "#### Flexibility: \n",
    "- Ensembles can be constructed from different types of models (e.g., decision trees, neural networks, support vector machines), allowing for a flexible and diverse approach to problem-solving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2a97b0-05e3-4cb7-aa0c-255a460c242e",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cf41bb-3d28-46d8-96ff-1b9e2c622423",
   "metadata": {},
   "source": [
    "##  Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d6a71-9d4d-4c01-917a-50529ed78430",
   "metadata": {},
   "source": [
    "## Answer\n",
    "#### Complexity: \n",
    "- Ensembles can be more complex and computationally intensive than individual models, requiring more resources and time for training and inference.\n",
    "#### Interpretability: \n",
    "- Individual models are often easier to interpret and understand. Ensembles, especially those with many models, can be more challenging to interpret.\n",
    "#### Diminishing Returns: \n",
    "- In some cases, the improvement in performance may not justify the increased complexity and computational cost. Simpler models might be sufficient for certain problems.\n",
    "#### Parameter Tuning: \n",
    "- Ensembles often require careful parameter tuning and selection of base models, which can be time-consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1801997-03ab-4707-8d65-e6f3e7237af5",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3a154a-4ecc-40f1-816f-608769eafc0c",
   "metadata": {},
   "source": [
    "## How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb8a42-128d-4c67-83ec-922b18329985",
   "metadata": {},
   "source": [
    "#### Original Sample: \n",
    "- Start with your original data sample.\n",
    "#### Bootstrap Samples: \n",
    "- Create many new samples (called bootstrap samples) by randomly selecting data points from your original sample with replacement. Each bootstrap sample should be the same size as the original sample.\n",
    "#### Calculate Statistic: \n",
    "- For each bootstrap sample, calculate the statistic you're interested in (e.g., the mean, median, etc.).\n",
    "#### Collect Results: \n",
    "- Collect the calculated statistics from all the bootstrap samples.\n",
    "#### Confidence Interval: \n",
    "- To find the confidence interval, sort the collected statistics and pick the values at the desired percentiles. For example, for a 95% confidence interval, pick the values at the 2.5th percentile and the 97.5th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0973171b-4b82-4020-9c8a-0c5ae43ada69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval: (14.5, 20.3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original data sample\n",
    "data = [10, 12, 13, 15, 16, 18, 20, 21, 23, 25]\n",
    "\n",
    "# Function to calculate the bootstrap confidence interval\n",
    "def bootstrap_confidence_interval(data, num_samples=1000, confidence_level=0.95):\n",
    "    # Generate bootstrap samples\n",
    "    bootstrap_samples = np.random.choice(data, size=(num_samples, len(data)), replace=True)\n",
    "    \n",
    "    # Calculate the statistic (mean) for each bootstrap sample\n",
    "    bootstrap_means = np.mean(bootstrap_samples, axis=1)\n",
    "    \n",
    "    # Calculate the percentiles for the confidence interval\n",
    "    lower_percentile = (1 - confidence_level) / 2\n",
    "    upper_percentile = 1 - lower_percentile\n",
    "    \n",
    "    # Get the confidence interval\n",
    "    lower_bound = np.percentile(bootstrap_means, lower_percentile * 100)\n",
    "    upper_bound = np.percentile(bootstrap_means, upper_percentile * 100)\n",
    "    \n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval = bootstrap_confidence_interval(data)\n",
    "print(f\"95% Confidence Interval: {confidence_interval}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f4e10d-a567-46e7-9c89-002f7c191ee6",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443d37c2-a37e-4125-899b-8e5a9fd8ad18",
   "metadata": {},
   "source": [
    "##  Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dadac4-1fd2-4479-bc53-4877b4aa728b",
   "metadata": {},
   "source": [
    "## Answer \n",
    "#### Bootstrap is a statistical technique used to estimate the distribution of a sample statistic by resampling with replacement from the original data. It's particularly useful when the theoretical distribution of the statistic is unknown or when the sample size is small. Here are the steps involved in bootstrap:\n",
    "### Steps Involved in Bootstrap\n",
    "#### Original Sample: \n",
    "Start with your original data sample. This is the dataset from which you want to estimate the distribution of a statistic (e.g., mean, median).\n",
    "#### Resampling: \n",
    "Create many new samples (called bootstrap samples) by randomly selecting data points from the original sample with replacement. Each bootstrap sample should be the same size as the original sample.\n",
    "#### Calculate Statistic: \n",
    "For each bootstrap sample, calculate the statistic you're interested in (e.g., the mean, median, standard deviation).\n",
    "#### Repeat: \n",
    "Repeat the resampling and calculation process a large number of times (e.g., 1,000 or 10,000 times) to generate a distribution of the statistic.\n",
    "#### Analyze Results: \n",
    "Analyze the distribution of the calculated statistics from all the bootstrap samples. You can use this distribution to estimate confidence intervals, standard errors, or other properties of the statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d9539c-6c22-403d-bbfe-3b88bd8900eb",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b046359-c1e8-4ca7-9ea3-851a8a12822a",
   "metadata": {},
   "source": [
    "## Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1434e3ef-b736-4590-9e51-4cca1853be4f",
   "metadata": {},
   "source": [
    "## Answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38944412-19a6-49fe-9a86-b858094d9e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given\n",
    "sample_mean = 15\n",
    "sample_size = 50\n",
    "sample_std = 2\n",
    "num_bootstraped_samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b6ad5d5-b317-4c98-9284-f03b72c4111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(loc=15, scale=2, size=50)\n",
    "def CI_bootstraped(data, confidence_interval=0.95, num_samples=10000):\n",
    "    bootstrap_samples = np.random.choice(data, size=(num_samples, len(data)), replace = True)\n",
    "    bootstrap_means = np.mean(bootstrap_samples)\n",
    "    lower_percentile = (1-confidence_interval)/2\n",
    "    upper_percentile = 1-lower_percentile\n",
    "    lower_bound = np.percentile(bootstrap_means, lower_percentile*100)\n",
    "    upper_bound = np.percentile(bootstrap_means, upper_percentile*100)\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f3bffc9-b2fc-4d99-ac1e-c2e4f18cbab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14.54660629577997, 14.54660629577997)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CI_bootstraped(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
