{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b962c2e1-bade-47ba-90e1-bb1a7c804a86",
   "metadata": {},
   "source": [
    "## Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2873c1c0-7eb4-43cc-b5f7-6abb464e0f68",
   "metadata": {},
   "source": [
    "### Euclidean Distance :\n",
    "- Straight Lines & Diagonal Movement Allowed\n",
    "### Manhattan Distance :\n",
    "- Only horizintal and vertical movements allowed.\n",
    "##\n",
    "### Performance of a KNN Classifier & Regressor :\n",
    "##### if the data is continuous and features are blended / highly correlated : \n",
    "- Go for Euclidean, not manhattan.\n",
    "#### If the target variable is symbolic, discrete or axis-aligned :\n",
    "- go for Manhattan, not Euclidean.\n",
    "#### if low dimension and low noise\n",
    "- Both Euclidean and Manhattan Plays good.\n",
    "#### High Dimension or Sparse Data :\n",
    "- Go for Manhattan, not Euclidean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e3c97-0247-48b8-9e9c-95a6bcf6b0c6",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28abfe-5320-40ea-a932-aaad6c8584f3",
   "metadata": {},
   "source": [
    "## Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc49c2c-9710-4666-8d95-5e16515bc888",
   "metadata": {},
   "source": [
    "#### Low K Value : Model may face Overfitting.\n",
    "#### High K value : Model may face Underfitting.\n",
    "- That's why we require an optimal Value of \"K\" for high accuracy model.\n",
    "##\n",
    "#### Techniques :\n",
    "##### 1. Grid Search CV : Automates searching through multiple k values and even distance metrics!\n",
    "##### 2. Plot on visualization / Elbow Method : Plot performance (accuracy or error) for a range of k values and look for the “elbow”—where improvement levels off.\n",
    "##### 3. Cross-Validation : Run KNN on multiple folds of the data and average the performance for each k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3749ef-66ab-4e1b-813f-d414eae640ad",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f0ad5-e283-4952-bb84-a15e03d7deea",
   "metadata": {},
   "source": [
    "## Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1796e1a-e24d-4c70-a3d2-8a277130c512",
   "metadata": {},
   "source": [
    "## Euclidean Distance :\n",
    "#### 1. When the data is continuous\n",
    "#### 2. Low dimension\n",
    "#### 3. Low Noise\n",
    "#### 4. Blended fatures / Hgihly Correlated\n",
    "## Manhattan Distance :\n",
    "#### 1. When the target variable is categorical\n",
    "#### 2. High Dimension\n",
    "##\n",
    "#### Use Euclidean Distance when:\n",
    "- Your features are continuous and measured on the same scale\n",
    "- You expect natural curved boundaries between classes\n",
    "- You’re predicting something like art style using blended color metrics\n",
    "#### Use Manhattan Distance when:\n",
    "- Your data has features that are independent or axis-aligned\n",
    "- You’re working with tabular, symbolic, or grid-based datasets\n",
    "- You want robustness in high-dimensional or sparse spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c239a3b2-fc85-44c2-bca4-40d6387fed22",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cf3474-1c26-4a5c-bceb-f6c46f143031",
   "metadata": {},
   "source": [
    "## Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c73bf1-45c4-41bc-ba9d-db4bac7d9e12",
   "metadata": {},
   "source": [
    "#### 1. n_neighbors\n",
    "- Small values → high variance, risk of overfitting.\n",
    "- Large values → more bias, risk of underfitting.\n",
    "##\n",
    "#### 2. Weights\n",
    "- 'uniform' good for clean data.\n",
    "- ( distance ) often better for noisy data—closer neighbors influence more.\n",
    "## \n",
    "#### 3. Algorithm\n",
    "### Affects runtime speed:\n",
    "- 'kd_tree' or 'ball_tree' work well with large, low-dimensional datasets.\n",
    "- (brute) computes distances manually—slow but simple.\n",
    "##\n",
    "#### 4. (distance metric)\n",
    "- p=1 (Manhattan) works better in high-dimensional or grid-like data.\n",
    "- p=2 (Euclidean) suited for smooth, continuous feature spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf21e9-00d6-4d84-8d85-c292917d32c4",
   "metadata": {},
   "source": [
    "## Techniques :\n",
    "1. Grid Search CV\n",
    "2. Cross - Validation\n",
    "3. Plot Accuracy / Visualizing\n",
    "4. Feature Scaling : Always scale features before KNN, since distance is scale-sensitive!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c3ecb9-0583-44d2-af8f-a1dc6f3bff74",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f947838-6ae0-4caa-bc87-d335a43be167",
   "metadata": {},
   "source": [
    "## Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de86012d-00a3-4df7-82a2-1e3445fae2e2",
   "metadata": {},
   "source": [
    "#### Too Small : \n",
    "- Not enough diversity<br>- High variance & poor generalization.\n",
    "#### Too Large : \n",
    "- Slower predictions<br>- More memory usage<br>- Possible noise overload.\n",
    "#### \n",
    "#### Accuracy & Responsiveness:\n",
    "##### More data = better generalization (up to a point).\n",
    "##### But too much irrelevant or noisy data can confuse KNN.\n",
    "##### Larger datasets slow down prediction time because KNN computes distances to all training samples for every query.\n",
    "#\n",
    "#### Techniques\n",
    "- FEATURE SCALING\n",
    "- DIMENSIONALITY REDUCTION\n",
    "- DATA SAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e833bf-9891-4a4e-8ef9-786b890056fc",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345ae52-689e-4631-926a-5fd224eb07c4",
   "metadata": {},
   "source": [
    "## Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed80e4e-022f-4bf5-8d74-17df5c5acc65",
   "metadata": {},
   "source": [
    "#### 1. Slow Predictions\t\n",
    "- KNN stores the entire training set and computes distances at runtime\t\n",
    "- Use BallTree or KDTree; consider approximate neighbors\n",
    "#### 2. Sensitive to Scale\t\n",
    "- Features with larger ranges dominate distance calculations\t\n",
    "- Normalize or standardize data (StandardScaler, MinMaxScaler)\n",
    "#### 3. Curse of Dimensionality\t\n",
    "- High-dimensional data makes all points feel equally far\t\n",
    "- Apply PCA or feature selection to reduce dimensionality\n",
    "#### 4. Noisy Neighbors\t\n",
    "- Outliers or mislabeled points can skew predictions\t\n",
    "- Use distance-weighted voting (weights='distance') or clean data\n",
    "#### 5. Tie Votes (Classification)\t\n",
    "- Equal number of neighbors from different classes\n",
    "- Use odd values of k or distance-based weighting\n",
    "#### 6. Struggles with Imbalanced Data\t\n",
    "- Majority class dominates neighbors\n",
    "- Try class rebalancing (SMOTE, undersampling), or adjust weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
