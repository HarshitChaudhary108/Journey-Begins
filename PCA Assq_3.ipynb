{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb61920d-287d-4147-aea9-27f173449a50",
   "metadata": {},
   "source": [
    "## Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach? Explain with an example.\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d082d33-dd0e-4945-a6fb-638711c27698",
   "metadata": {},
   "source": [
    "#### An eigenvector of a matrix is a special vector whose direction doesn‚Äôt change when that matrix is applied to it.\n",
    "#### An eigenvalue tells you how much the eigenvector is stretched or shrunk.\n",
    "### Eigen-decomposition is the process of breaking down a square matrix into its eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56900fb7-e8c6-46a6-8db6-09f4f887e4b2",
   "metadata": {},
   "source": [
    "## In PCA:\n",
    "#### The covariance matrix of your data is decomposed via eigen-decomposition.\n",
    "#### Eigenvectors become your principal components.\n",
    "#### Eigenvalues tell you how much variance each component captures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2e7a72-1e29-4eba-a5b6-aa80f11db28c",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e563d65-ed6b-41c1-bd04-95f64c7dec61",
   "metadata": {},
   "source": [
    "## Q2. What is eigen decomposition and what is its significance in linear algebra?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8712d262-154f-404a-aac1-43cd7f635e6d",
   "metadata": {},
   "source": [
    "#### Eigen-decomposition (also called spectral decomposition) is the process of breaking a square matrix into parts defined by its eigenvalues and eigenvectors.\n",
    "#### Significance : \n",
    "##### 1. Dimensionality Reduction.\n",
    "##### 2. Diagonalization\n",
    "- It transforms a matrix into its diagonal form, which is much easier to work with ‚Äî especially for functions like exponentials (ùëí raise to power ùê¥), used in quantum mechanics and finance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcca1e5-d366-4ca6-a8ad-c2800a6bb2ae",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bcc122-dfa8-427e-a5b1-a0ab4b4174bc",
   "metadata": {},
   "source": [
    "## Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach? Provide a brief proof to support your answer.\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79312a-888b-4555-8e3c-fcb07f6764a6",
   "metadata": {},
   "source": [
    "#### - A square matrix is diagonalizable if and only if it has n linearly independent eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d072bbb-9930-4628-a5c4-5c8bcc7aa630",
   "metadata": {},
   "source": [
    "- Note : Symmetric matrices are always diagonalizable ‚Äî they have orthonormal eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d92a4-d30b-456f-a4cf-68a89ca930a2",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad33cf-fd09-4277-be2d-b092250150ac",
   "metadata": {},
   "source": [
    "## Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach? How is it related to the diagonalizability of a matrix? Explain with an example.\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa751fa-20a0-4b75-b2aa-9a46b24b5341",
   "metadata": {},
   "source": [
    "#### The spectral theorem is like the golden rulebook for symmetric matrices ‚Äî and it‚Äôs what makes eigen-decomposition so reliable in practical algorithms like PCA, physics simulations, and even vibration analysis.\n",
    "### The spectral theorem says:\n",
    "    - ‚ÄúEvery real symmetric matrix can be diagonalized by an orthogonal matrix.‚Äù\n",
    "### Significance in Eigen-Decomposition\n",
    "#### It guarantees:\n",
    "- Diagonalizability of real symmetric matrices\n",
    "- Eigenvalues are real numbers\n",
    "- Eigenvectors can be chosen to be orthogonal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a84a866-bc68-41f0-a6e0-b8da149235cd",
   "metadata": {},
   "source": [
    "##### PCA: \n",
    "- We eigen-decompose the covariance matrix (which is always symmetric!)\n",
    "##### Stability analysis: \n",
    "- Real eigenvalues ‚Üí real behaviors\n",
    "##### Computational efficiency: \n",
    "- Orthogonal matrices simplify matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838cfec-a6ae-46f3-9589-ef16ee09ab73",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e893752-6933-4822-9557-ce9dd75d71ac",
   "metadata": {},
   "source": [
    "## Q5. How do you find the eigenvalues of a matrix and what do they represent?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faedc42-4263-4ab7-9a25-2f6accfcd19b",
   "metadata": {},
   "source": [
    "#### If ùê¥ is an ùëõ √ó ùëõ square matrix, the eigenvalues ùúÜ are found by solving : det(ùê¥‚àíùúÜùêº)=0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d471e-fe40-4546-964a-3405f36faa51",
   "metadata": {},
   "source": [
    "#### What they represent : They tell you how a transformation acts along its own natural directions (eigenvectors).\n",
    "#### Behavior Insight:\n",
    "    - Positive eigenvalue ‚Üí stretches in the same direction\n",
    "    - Negative eigenvalue ‚Üí flips (reflection)\n",
    "    - Zero eigenvalue ‚Üí collapses (loses dimension!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c5659a-fcc6-4529-92f5-7d57e0864aa0",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf75a4b-4cb4-4c45-867c-9a3ea56f0749",
   "metadata": {},
   "source": [
    "## Q6. What are eigenvectors and how are they related to eigenvalues?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523cc249-b739-4a0d-8d7b-7a80f3fb5dee",
   "metadata": {},
   "source": [
    "#### Eigen Vectors : Eigen vectors the direections of the PCA's.\n",
    "#### Relation :\n",
    "    - When you apply a matrix transformation to its eigenvector, the result is just that same vector scaled by its eigenvalue.\n",
    "    - In essence: Matrix √ó Eigenvector = Eigenvalue √ó Same Eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0112b6a3-5f5c-49e8-b918-227a2a4d4c5e",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a1eb34-0ec5-4a77-a650-51674f519766",
   "metadata": {},
   "source": [
    "## Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49b3f4b-fef4-4859-8801-cfeee091b790",
   "metadata": {},
   "source": [
    "#### Eigen Vector : \n",
    "    - These are special directions in space that don‚Äôt change direction when the transformation (represented by a matrix) is applied.\n",
    "#### Eigen Values :\n",
    "    - Eigenvalues: Tell you how much the eigenvector gets stretched (or shrunk).\n",
    "        - If the eigenvalue is:\n",
    "        - Greater than 1: The vector is stretched.\n",
    "        - Between 0 and 1: The vector is contracted.\n",
    "        - Negative: The vector is flipped in direction.\n",
    "        - Zero: The vector is flattened to the origin (loses its identity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d476ebc-87a9-4c01-9f66-78bd4a72b327",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f3b9c-655a-4abd-b727-5df555e520f7",
   "metadata": {},
   "source": [
    "## Q8. What are some real-world applications of eigen decomposition?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c63f0-164e-43b3-a6ee-60084b235170",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis (PCA): \n",
    "- Eigen decomposition is central to PCA, a technique for dimensionality reduction.\n",
    "#### Signal Processing: \n",
    "- Eigen decomposition is used in filtering signals and image compression.\n",
    "#### Facial Recognition: \n",
    "- Eigenfaces use eigen decomposition of facial image datasets to extract key features for recognition.\n",
    "#### Natural Language Processing: \n",
    "- Word embeddings and co-occurrence matrices sometimes benefit from eigen decomposition for dimensionality reduction and semantic analysis.\n",
    "##### and more...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae102c0-7700-4490-bcc5-24ef6bc74f3a",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2251537a-6b07-4c68-9a09-80e9f58c420a",
   "metadata": {},
   "source": [
    "## Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c60ce1-19e1-44d1-b3cf-f87ebea0bdf6",
   "metadata": {},
   "source": [
    "#### Unique Eigenvalues and Eigenvectors\n",
    "- For most matrices, each distinct eigenvalue has its own set of corresponding eigenvectors. \n",
    "- If the matrix is diagonalizable, it will have a full set of linearly independent eigenvectors,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a1fdd9-765f-4447-a2e6-6244d487dc9b",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04167a7f-3cc4-4b5d-98d4-309cc5ae9a61",
   "metadata": {},
   "source": [
    "## Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning? Discuss at least three specific applications or techniques that rely on Eigen-Decomposition.\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b068f0-1867-4563-8257-d2ebeb45448e",
   "metadata": {},
   "source": [
    "#### 1. Principal Component Analysis (PCA)\n",
    "- Purpose: \n",
    "    - Dimensionality reduction while preserving variance.\n",
    "#### 2. Spectral Clustering\n",
    "- Purpose: \n",
    "    - Cluster data with complex boundaries (non-convex).\n",
    "#### 3. Linear Discriminant Analysis (LDA)\n",
    "- Purpose: \n",
    "    - Supervised dimensionality reduction for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e697c6d-8323-4495-bc0a-7a77ff759a12",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913fb3dc-6b23-46d2-8132-dc771938a6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c50d504-70e5-47c9-aa52-2fcef692d511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127476e0-1c00-4338-9ba6-26041501b880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a4808-93c3-443d-934e-d9392370c349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03949461-30b0-4cb3-a83a-7bc5ff32f08d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7db623-fd45-4613-9666-5eb77d042dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
