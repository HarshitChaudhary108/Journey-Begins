{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e3effb9-31a3-4b7b-bf56-6a5d414f1bb9",
   "metadata": {},
   "source": [
    "## Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e10506-b869-404e-9a3e-b650bec30c11",
   "metadata": {},
   "source": [
    "#### OLS (ordinary least squares) regression finds the best fit of line by minimizing the sum of the squared difference between the predicted values (predicted by line) and the actual values. But it has a problem that if the features of the dataset hare correlated to each other, it can cause over fitting.\n",
    "#### On the other hand, Ridge Regression is similar to OLD but it has an advantage that it adds a penalty to the size of the coefficient. That prevents overfitting by shrinking the coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8a9bc-528e-4647-a805-2746703bad2c",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ace72d7-e967-44c5-8fb5-07b12c70b8be",
   "metadata": {},
   "source": [
    "## Q2. What are the assumptions of Ridge Regression?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1da5bb-cd34-4621-a4fb-a4364bbad395",
   "metadata": {},
   "source": [
    "### Assumptions :\n",
    "- The relationship between the independent and dependent features should be a straight line.\n",
    "- Each data point or feature should be independent to each other.\n",
    "- the difference between the predicted and actual values is constant across all the levels of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9949c1c4-8fe6-466a-b7b0-5b7b3e16059a",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34aa130-3fd7-415f-8ba4-d9a41e380fd5",
   "metadata": {},
   "source": [
    "## Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326c4bd7-1de7-4e68-a9db-5f9e5c6d82a7",
   "metadata": {},
   "source": [
    "### Finding the Best Lambda: To find the best value for lambda, we use a technique called cross-validation. This involves:\n",
    "- Splitting the data into several parts.\n",
    "- Training the model on some parts and testing it on the remaining part.\n",
    "- Repeating this process multiple times with different lambda values.\n",
    "- Choosing the lambda that gives the lowest error on the test data.\n",
    "### Practical Steps :\n",
    "- Standardize the Data: Ensure all predictor variables have a mean of 0 and a standard deviation of 1.\n",
    "- Use Cross-Validation: Use functions like cv.glmnet in R or similar in Python to automatically perform cross-validation and find the optimal lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa291223-0e59-46b2-b4d2-ec55e7f7b685",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08660ec0-c3a8-4c95-a2ac-4ea76212c5ef",
   "metadata": {},
   "source": [
    "##  Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f3795-925d-4a3e-9951-a1e733c5960d",
   "metadata": {},
   "source": [
    "#### No, Ridge regression is typically not used for feature selection. Ridge regression shrinks the coefficients of less important features but it do not get them completely 0. By shrinking the coefficients, Ridge regression can make the model more stable and less sensative to the noise in the data.\n",
    "#### This can indirectly help us to identify the less important features because the less important features will have smaller coeffients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179256a-6538-44db-bd18-eaea595b732c",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fafa69-9a38-4b65-9a45-bdb33782d489",
   "metadata": {},
   "source": [
    "##  Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2110ab-a374-4e54-bc91-c2445e83e640",
   "metadata": {},
   "source": [
    "#### Ridge Regression adds a penalty term to the ordinary least squares (OLS) loss function. By adding this penalty, Ridge Regression shrinks the coefficients of less important predictors towards zero but never exactly to zero.\n",
    "#### In the presence of multicollinearity, the OLS estimates can become highly sensitive to changes in the model. Ridge Regression stabilizes these estimates by imposing a constraint on the size of the coefficients, leading to more reliable and interpretable models.\n",
    "#### While Ridge Regression introduces some bias into the model, it significantly reduces the variance, leading to better overall performance, especially when dealing with multicollinear data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a01345-46ed-41e8-9de7-2a447d1ea4fc",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1f1c24-4c46-49cc-aded-f3a2dc31c83c",
   "metadata": {},
   "source": [
    "##  Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ef764-e6a3-45e7-9dc6-59410d95c2cc",
   "metadata": {},
   "source": [
    "## Yes, Ridge Regression can handle both categorical and continuous independent variables. However, you need to convert categorical variables into a numerical format before using them in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1a9583-d4c3-4cc2-bf93-2d038c84e723",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c200a52-f5e3-437e-8fe3-72823e479079",
   "metadata": {},
   "source": [
    "## Q7. How do you interpret the coefficients of Ridge Regression?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f61c0-93bb-4168-9b81-af130d3251f5",
   "metadata": {},
   "source": [
    "### Magnitude: \n",
    "- The size of a coefficient indicates the strength of the relationship between the independent variable and the dependent variable. Larger absolute values mean a stronger relationship.\n",
    "### Direction: \n",
    "- The sign of the coefficient (positive or negative) shows the direction of the relationship. A positive coefficient means that as the independent variable increases, the dependent variable also increases. A negative coefficient means the opposite.\n",
    "### Shrinkage: \n",
    "- Due to the penalty term, Ridge Regression coefficients are generally smaller than those in ordinary linear regression. This shrinkage helps to reduce the impact of less important variables and makes the model more robust1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40cf306-e5d5-4a23-85e1-a613814f5f05",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d3a7c-491c-4049-a6fe-30db67df6b86",
   "metadata": {},
   "source": [
    "## Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460dd758-faf7-4736-a4e3-162967a816cf",
   "metadata": {},
   "source": [
    "### Data Preparation: \n",
    "- First, you need to prepare your time-series data. This often involves creating lagged variables, which are past values of the time series that help predict future values.\n",
    "### Model Training: \n",
    "- Use Ridge Regression to train your model on the prepared data. The Ridge Regression model will learn the relationship between the lagged variables and the target variable (future values).\n",
    "### Regularization: \n",
    "- Ridge Regression includes a penalty term that helps prevent overfitting, which is especially useful in time-series data where patterns can be complex and noisy.\n",
    "### Prediction: \n",
    "- Once trained, the model can be used to make predictions on future data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b9451-bc5f-4956-903a-06fc760011f5",
   "metadata": {},
   "source": [
    "#### In simple terms, Ridge Regression helps to make more reliable predictions by controlling for overfitting, which is crucial for time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bee97ce-bda6-4b76-89e3-3a550619277a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[301.13286497 340.58675782 419.49454352 409.63107031 232.08855248\n",
      " 389.90412389 360.31370425]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theha\\AppData\\Local\\Temp\\ipykernel_18812\\227793379.py:7: FutureWarning: 'Me' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  'Month': pd.date_range(start='1/1/2020', periods=36, freq='Me'),\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Month': pd.date_range(start='1/1/2020', periods=36, freq='Me'),\n",
    "    'Sales': [100, 120, 130, 150, 170, 160, 180, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create lagged variables\n",
    "df['Lag1'] = df['Sales'].shift(1)\n",
    "df['Lag2'] = df['Sales'].shift(2)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Features and target\n",
    "X = df[['Lag1', 'Lag2']]\n",
    "y = df['Sales']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Ridge Regression model\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Predict future sales\n",
    "predictions = ridge.predict(X_test)\n",
    "\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
