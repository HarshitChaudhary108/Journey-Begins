{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a706fe1-35e7-42f7-a738-d73cd5b16143",
   "metadata": {},
   "source": [
    "## Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c244af-3a00-44b7-9bdf-c29607f5b091",
   "metadata": {},
   "source": [
    "#### Lasso regression is a model used for future predictions. It reduces the coefficients of the not important feature to the zero.\n",
    "#### It's differ from othe regression techniques because it gives us the option of Feature selection which other regression models can't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac8a538-c205-4891-b715-35445d5523cc",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca40e523-febc-4cc1-8e54-bfc3398d3516",
   "metadata": {},
   "source": [
    "## Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2057f3a4-381b-4426-872c-a686eaef7724",
   "metadata": {},
   "source": [
    "- The main advantage of using Lasso Regression for feature selection is that it can automatically identify and keep only the most important features while eliminating the less important ones. It does this by shrinking the coefficients of less important features to exactly zero, effectively removing them from the model. This makes the model simpler, more interpretable, and helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe22dc9d-02b3-42af-885b-afb7a1f8ad34",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a53c440-e36c-4de2-ad64-bdc9c404b780",
   "metadata": {},
   "source": [
    "##  Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e289be8-9ff1-4151-bebb-eb1da87f0849",
   "metadata": {},
   "source": [
    "#### Zero Coefficients:\n",
    "- If a coefficient is exactly zero, it means that the corresponding predictor variable has been excluded from the model. Lasso Regression does this automatically to simplify the model by removing less important variables.\n",
    "#### Non-Zero Coefficients: \n",
    "- If a coefficient is not zero, it indicates that the predictor variable has an impact on the response variable. The sign (positive or negative) of the coefficient tells you the direction of the relationship:\n",
    "    - Positive Coefficient: As the predictor variable increases, the response variable also increases.\n",
    "    - Negative Coefficient: As the predictor variable increases, the response variable decreases.\n",
    "#### Magnitude of Coefficients: \n",
    "- The size of the non-zero coefficients indicates the strength of the relationship. Larger absolute values suggest a stronger influence on the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92efb7e8-2dc3-42f7-9649-cdce2b437e9e",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eb50ed-4bf2-4b7d-89f8-9ef915189661",
   "metadata": {},
   "source": [
    "## Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9285c68e-8815-4763-bbb1-4e915fa3fc53",
   "metadata": {},
   "source": [
    "#### Lambda (λ): \n",
    "- This parameter controls the strength of the penalty applied to the coefficients. Think of it as a knob that you can turn to adjust how much you want to shrink the coefficients towards zero.\n",
    "    - Small λ: When λ is small, the penalty is weak, and the model behaves more like a regular linear regression, keeping most of the coefficients as they are.\n",
    "    - Large λ: When λ is large, the penalty is strong, and the model shrinks more coefficients to zero, effectively removing some predictor variables from the model.\n",
    "#### Effect on Model Performance:\n",
    "- Overfitting:\n",
    "    - If λ is too small, the model might include too many variables, leading to overfitting (where the model performs well on training data but poorly on new data).\n",
    "- Underfitting:\n",
    "    - If λ is too large, the model might exclude too many variables, leading to underfitting (where the model is too simple and misses important patterns in the data).\n",
    "- Optimal λ:\n",
    "    - The goal is to find a balance where λ is just right, providing a model that generalizes well to new data by including only the most important variables1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d3a49-9808-4f1f-b889-cab86c3aca4d",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd8f46-a781-463f-84c7-86ccfa662bb1",
   "metadata": {},
   "source": [
    "## Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ce50ea-e2a2-446f-bba1-a111f6f967f6",
   "metadata": {},
   "source": [
    "#### Yes, Lasso Regression can be used for non-linear regression problems, but it requires some adjustments. Here’s a simple explanation:\n",
    "#### Lasso Regression Basics: \n",
    "- Lasso Regression is a linear model that helps in feature selection by shrinking some coefficients to zero, effectively removing less important features.\n",
    "#### Handling Non-Linearity: \n",
    "- To use Lasso Regression for non-linear problems, you can transform the input features into a higher-dimensional space where the relationship becomes linear. This is done using techniques like polynomial features or interaction terms.\n",
    "###### Example: \n",
    "        - Suppose you have a feature ( x ) and you suspect a non-linear relationship with the target variable ( y ). You can create new features like ( x^2 ), ( x^3 ), etc., and then apply Lasso Regression on these transformed features.\n",
    "- Implementation: In practice, you can use libraries like scikit-learn in Python to easily create polynomial features and apply Lasso Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764bd671-700d-4a6c-9e80-33e7bc56adc6",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0239b27-3658-4316-a596-122c4b3d50c2",
   "metadata": {},
   "source": [
    "##  Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dcdcce-b3be-4ddb-b276-5ca1873ce335",
   "metadata": {},
   "source": [
    "#### Ridge Shrinks the coefficients of not important features towards 0 but not zero.\n",
    "#### Lasso Shrinks the coeffients of not important features towards zero, effectively 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a660ad0-914b-476b-b193-d2aa67e44b5a",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e374d45-2c63-49f1-9179-0d06079717ac",
   "metadata": {},
   "source": [
    "##  Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cdcc08-260c-4a86-8b9b-f6a940c6e6cf",
   "metadata": {},
   "source": [
    "#### This occurs when two or more input features are highly correlated, making it difficult to determine their individual effects on the target variable. Lasso Regression adds a penalty to the regression model that shrinks some coefficients to zero. This means it can effectively select one feature from a group of highly correlated features, reducing redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47d94f-1e4f-4f40-8a65-a33512e80790",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae17ca65-8ef6-487c-9649-4970e7b7ea0b",
   "metadata": {},
   "source": [
    "## Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "## Answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12630a-ac4b-436a-9063-cd0a86ad58dc",
   "metadata": {},
   "source": [
    "#### This is the most common method. You split your data into several parts (folds), train the model on some parts, and test it on the remaining part. You repeat this process multiple times with different lambda values and choose the one that gives the best performance (e.g., lowest mean squared error).\n",
    "#### Grid Search: \n",
    "- This involves trying out a range of lambda values (e.g., 0.01, 0.1, 1, 10) and using cross-validation to evaluate each one. The lambda that results in the best cross-validation score is chosen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
